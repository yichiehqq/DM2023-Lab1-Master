{"cells":[{"cell_type":"markdown","metadata":{"id":"FDNTc1wFid-1"},"source":["### Student Information\n","Name: 林奕杰\n","\n","Student ID: 109020010\n","\n","GitHub ID: yichiehqq"]},{"cell_type":"markdown","metadata":{"id":"-BajKEF_id-5"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"rSzgpPbVid-6"},"source":["### Instructions"]},{"cell_type":"markdown","metadata":{"id":"UjbdfsDcid-6"},"source":["1. First: do the **take home** exercises in the [DM2023-Lab1-Master](https://github.com/fjrialdnc0615/DM2023-Lab1-Master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n","\n","\n","2. Second: follow the same process from the [DM2023-Lab1-Master](https://github.com/fjrialdnc0615/DM2023-Lab1-Master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n","    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. You need to combine three labeled datasets into one file for your data preparation part.\n","    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n","\n","\n","3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n","    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n","    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n","    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n","\n","\n","4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n","\n","\n","5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n","\n","\n","You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fjrialdnc0615/DM2023-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Thursday)__. "]},{"cell_type":"markdown","metadata":{},"source":["# Second Part"]},{"cell_type":"markdown","metadata":{},"source":["## 2.I Data Preparation"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":285,"status":"ok","timestamp":1666241724343,"user":{"displayName":"Kevin Yang","userId":"04518680380941289042"},"user_tz":-480},"id":"IDH6foBIid-9"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2748, 3)\n","(2748, 3)\n"]}],"source":["### Begin Assignment Here\n","import pandas as pd\n","import numpy as np\n","\n","filepath_dict = {'amazon': 'amazon_cells_labelled.txt',\n","                 'imdb': 'imdb_labelled.txt',\n","                 'yelp': 'yelp_labelled.txt'}\n","\n","df_list = []\n","for source, filepath in filepath_dict.items():\n","    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n","    df['source'] = source\n","    df_list.append(df)\n","\n","df_unshuffled = pd.concat(df_list)\n","\n","\n","# shuffel \n","seed = 42\n","df = df_unshuffled.sample(frac=1, random_state=seed)\n","# Reset the index\n","df.reset_index(drop=True, inplace=True)\n","\n","print(df_unshuffled.shape)\n","print(df.shape)\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                                            sentence  label  source\n","0  It's close to my house, it's low-key, non-fanc...      1    yelp\n","1  If you stay in Vegas you must get breakfast he...      1    yelp\n","2  Let's start with all the problemsthe acting, ...      0    imdb\n","3  It's too bad that everyone else involved didn'...      0    imdb\n","4  i felt insulted and disrespected, how could yo...      0    yelp\n","5  Yet Plantronincs continues to use the same fla...      0  amazon\n","6  Whatever prompted such a documentary is beyond...      0    imdb\n","7  Any grandmother can make a roasted chicken bet...      0    yelp\n","8         Do NOT buy if you want to use the holster.      0  amazon\n","9  I ordered this product first and was unhappy w...      0  amazon\n"]},{"data":{"text/plain":["0       1\n","1       1\n","2       0\n","3       0\n","4       0\n","       ..\n","2743    0\n","2744    0\n","2745    1\n","2746    1\n","2747    0\n","Name: label, Length: 2748, dtype: int64"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["print(df[:10])"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["It's close to my house, it's low-key, non-fancy, affordable prices, good food.\n","If you stay in Vegas you must get breakfast here at least once.\n","Let's start with all the problemsthe acting, especially from the lead professor, was very, very bad.  \n","It's too bad that everyone else involved didn't share Crowe's level of dedication to quality, for if they did, we'd have a far better film on our hands than this sub-par mess.  \n","i felt insulted and disrespected, how could you talk and judge another human being like that?\n"]}],"source":["for i in range(5):\n","    print(df.sentence[i])"]},{"cell_type":"markdown","metadata":{},"source":["## 2.II Data Transformation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","# my functions\n","import helpers.data_mining_helpers as dmh\n","\n","score = ['postive', 'negative']\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["PQPCUbx1ie4R","8qg4up1B_EhD","lC7ymUlG_fai","xtvWLH1x_7nV","bgULadKFBXL-","SZ4rgA1mBir5","9VirxMl6CGN2","yoTS9Vh8ESzB","NGVM3wSjFt7v","bH9BQLSWF9Uf","y8I6L8Z8JGsv","TFIl1hpMJqnv","DobYRQ4FLetu","9pfemrkcLiUG"],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
